% !TeX root = ./cs525.tex
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage[numbers,sort,compress]{natbib}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}

\usepackage{algorithmic}
\usepackage{algorithm}

\newlength\myindent % define a new length \myindent
\setlength\myindent{6em} % assign the length 2em to \myindet
\newcommand\bindent{%
  \begingroup % starts a group (to keep changes local)
  \setlength{\itemindent}{\myindent} % set itemindent (algorithmic internally uses a list) to the value of \mylength
  \addtolength{\algorithmicindent}{\myindent} % adds \mylength to the default indentation used by algorithmic
}
\newcommand\eindent{\endgroup} % closes a group
\cvprfinalcopy % *** Uncomment this line for the final submission

\begin{document}

%%%%%%%%% TITLE
\title{Final Report : CS 525 - Spring 2021\\
Asynchronous Federated Learning on Hierarchical Clusters}

\author{First Author djkdlkdlkdk\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}


\and
Third Author\\
Institution3\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
% \thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
%Federated Learning (FL) is an approach for machine learning tasks over a large corpus of distributed data. When federated networks are composed of a massive number of devices, communication is a critical challenge in federated networks. In this project, we will tackle this problem by exploring different architectural patterns for the design of federated learning systems. Clustering Federated Learning could be potentially used to form an optimal system pattern with peer-to-peer communication between devices utilized to reduce the communication load between devices and the server.
Federated Learning has been a hot field of research and industry application for many years. However, the typical federating patterns has some intrisic issues such as heavy server traffic, long period of convergence, and unreliable accuracy. In this paper, we will try to address these issues by using a brand new pattern: asynchronous hierarchical federated learning. In AsyncHierFed Learning, the central server uses either the network topology or some clustering algorithm to assign clusters for workers (i.e., client devices). In each cluster, a special aggregator device is selected to enable hierarchical learning, leads to efficient communication between server and workers, so that the burden of the server can be significantly reduced. In addition, asynchronous federated learning schema is used to tolerate heterogeneity of the system and achieve fast convergence, i.e., the server aggregates the gradients from the workers weighted by a staleness parameter to update the global model, and regularized stochastic gradient descent is performed in workers. 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
% Federated Learning (FL) is a decentralized collaborative machine learning schema for privacy preserving. 


% Key idea:
% \begin{itemize}
%     \item Server use clustering algorithm to assign clusters, as well as a special coordinator device in each cluster.
%     \item Asynchronous federated learning schema is used, i.e., gradient update with a staleness parameter in server, and regularized SGD in clients. 
%     \item Hierarchical learning leads to efficient communication between server and clients with the assistance of the coordinator of each cluster, significantly reduces the burden of the server. 
% \end{itemize}

Federated learning is a machine learning methodology where the model updates happen in many clients (mobile devices, silos, etc.), as opposed to a more traditional setting, where training happens on a centralized server. This approach has many benefits, such as avoiding the upkeep of a centralized server, minimizing the network traffic between many clients and servers, as well as keeping sensitive user data anonymous and private. 

In 2016, McMahan et al.\cite{2016arXiv160205629B} defined the term Federated Learning: “We term our approach Federated Learning, since the learning task is solved by a loose federation of participating devices (which we refer to as clients) which are coordinated by a central server.” For cross-device Federated Learning, the most common architecture pattern is a centralized topology. This topology has been a popular area of research, and has seen much success when applied to commercial products. However, the centralized topology introduces some challenges. 

We are learning from the tradeoffs presented by the comparison of certain topologies and help us focus on which tradeoffs we want to measure specifically.
In a centralized network topology, a central server is where all the training happens. However, in some scenarios, a central server may not always be desirable; or the central may not be powerful enough \cite{2016arXiv161005202V}. The central server could also become a bottleneck for large amount of network traffic, and large number of connections. This bottleneck is exacerbated when the federated networks are composed of a massive number of devices \cite{2017arXiv170509056L}. Furthermore, current Federated Learning algorithms, such as Federated Averaging, can only efficiently utilize hundreds of devices in each training round, but many more are available \cite{2019arXiv190201046B}

One approach that attempted to address these challenges is decentralized training \cite{2016arXiv160205629B,2017arXiv170509056L,2018arXiv180307068T,2016arXiv161005202V}. However, in large scale FL systems, a fully decentralized FL topology is inefficient, since the convergence time could be long, and the traffic between devices could be too intensive. Well-connected or denser networks encourage faster consensus and give better theoretical convergence rates, which depend on the spectral gap of the network graph. However, when data is IID, sparser topologies do not necessarily hurt the convergence in practice. Denser networks typically incur communication delays which increase with the node degrees \cite{2019arXiv191204977K}. 
We take a hard look at the gossip learning, one of these state-of-the-art decentralized machine learning protocols. Because this paper identifies the conditions in which gossip learning can and cannot be applied, we take lessons learned in our evaluation of other topologies including gossip learning.

We look into segmented network capacity and focus on utilizing the bandwidth given the restraints of mobile networks.
In this paper, we examine how clustered network topologies could have an effect on several key measures compared to fully decentralized federated learning topology. Some key measures we look at include: client-server network traffic, convergence time, and model accuracy. In the clustered network topology, each client will belong to a cluster. Each cluster will also have a leader, so there is still a notion of a central server. However, the responsibilities of the central server are limited to initiating a new round of training, and talking to leaders to collect updated models. In this topology, clients do not communicate directly with the central server. This should significantly reduce the amount of network ingress incurred at the central server. We are also able to observe some interesting results on the convergence time after adopting the clustered topology. 

There has been some existing work \cite{9207469,2019arXiv190606629G,9174890,9016505} on clustered network topology. However, they are mainly focused on model performance and accuracy under heterogeneous data distribution. In this paper, we examine the effect of clustered network topology on the system performance of federated learning systems. Specifically, we dive into the effect on client-server network bandwidth, model convergence time, and model accuracy. We also explore different architectural patterns for clustered federated learning and their effects on the key measurements mentioned above.

The paper is structured with different sections. In the Introduction section, we go through some background on federated learning as well as a high level description of the work we are doing. In the Related Work section, we go through some work done in the field of clustering federated learing, and compare them to the work we are doing. In the Approach section, we are describing in details how we approach this problem. This includes a problem formulation, different patterns for clustering, as well as a convergence analysis. Then we present our experiment results. In the last section, we discuss our findings and present future work.

\section{Related Work}

%\cite{bonawitz2019towards} describes a high-level design of scalable production systems for Federated Learning in the domain of mobile devices, based on TensorFlow.
%Some unique characteristics and challenges of federated learning are discussed in \cite{li2020federated}. It provides a broad overview of current approaches, and outlines several directions of future work.
%\cite{lo2021architectural} presents a collection of architectural patterns to deal with the design challenges of federated learning systems. Especially, the Client Cluster pattern is the one we want to utilize.

%Gossip learning is a decentralized alternative to federated learning. 
%\cite{hegedHus2019gossip} presents a thorough comparison of Centralized Federated Learning and Gossip Learning. Examine the aggregated cost of machine learning in both cases, considering also a compression technique applicable in both approaches.
%\cite{lalitha2019peer} presents a peer-to-peer Federated Learning on graphs, which is a distributed learning algorithm in which nodes update their belief by judicially aggregating information from their local observational data with the model of their one-hop neighbors to collectively learn a model that best fits the observations over the entire network.
%Coral is a peer-to-peer self-organizing content distribution system introduced by \cite{freedman2003sloppy}. Coral creates self-organizing clusters of nodes that fetch information from each other to avoid communicating with more distant or heavily-loaded servers. 
%As a peer-to-peer FL framework particularly targeted towards medical applications, BrainTorrent introduced in \cite{roy2019braintorrent} presents a highly dynamic peer-to-peer environment, where all centers directly interact with each other without depending on a central body.


The most widely used and straightforward algorithm to aggregate the local models is to take the average, proposed in \cite{mcmahan2017communication} and known as Federated Averaging (\textit{FedAvg}).
In FL, the communication cost often dominates the computation cost \cite{mcmahan2017communication}, thus is one of the key issues we need to resolve for implementing FL system at scale. In particular, the state-of-the-art deep learning models are designed to achieve higher prediction performance at the cost of increasing model complexity with millions or even billions of parameters \cite{devlin2018bert, brown2020language}. On the other hand, FL requires frequent communication of the models between the server and workers. As such, \textit{FedAvg} \cite{mcmahan2017communication} encourages each worker to perform more iterations of local updates before communicating during global aggregation, this results in significantly less communication rounds, and also increases the accuracy eventually as model averaging produces regularization effect. Another way to decrease the communication cost is to reduce the size of model information that needs to be sent, either through model compression techniques such as sparsification \cite{stich2018sparsified} and quantization \cite{caldas2018expanding}, or only select a small portion of important gradients to be communicated \cite{tao2018esgd} based on the observation that most of deep learning model parameters are closed to zero \cite{strom2015scalable}.  However, these methods may result in deterioration of model accuracy, or incur high computation cost \cite{lim2020federated}. Alternatively, \cite{liu2020client} proposed client-edge-cloud hierarchical federated learning (\textit{HierFAVG}), an edge computing paradigm in which the edge servers play the roles of intermediate parameter aggregators. The hierarchical FL algorithm leverages on the proximity of edge servers, significantly relieves the burden of the central server on remote cloud. 
\cite{yuan2020hierarchical} introduces a hierarchical federated learning protocol through LAN-WAN orchestration, which involves a hierarchical aggregation mechanism in the local-area network (LAN) due to its abundant bandwidth and almost negligible monetary cost than WAN, and incorporates cloud-device aggregation architecture, intra-LAN peer-to-peer (p2p) topology generation, inter-LAN bandwidth capacity heterogeneity.
While the hierarchical learning pattern is promising to reduce communication, it is not applicable to all networks, as the physical hierarchy may not exist or be known \textit{a priori} \cite{li2020federated}. 

\cite{sattler2020clustered} designs and implements Clustered Federated Learning (CFL) using a cosine-similarity-based clustering method that creates a bi-partitioning to group client devices with the same data generating distribution into the same cluster.  Client devices are clustered into different groups according to their properties. It has better performance for the non-IID-severe client network, without accessing the local data.
\cite{briggs2020federated} implemented a hierarchical clustering step (FL+HC) to separate clusters of clients by the similarity of their local updates to the global joint model. Once separated, the clusters are trained independently and in parallel on specialised models.
In \cite{nguyen2020self}, a self-organizing hierarchical structured FL mechanism is implemented based on democratized learning, agglomerative clustering, and hierarchical generalization.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cite{hegedHus2021decentralized} contributes to the field of FL with a comparison of the efficiency of decentralized and centralized solutions that are based on keeping the data local, with an assumption that gossip learning will hurt performance.  They recognize many areas of tradeoffs to measure, and that many algorithms can outperform each other depending on what metric they are measuring.  However in their testing, they observed that federated learning converges faster, since it can mix information more efficiently and is clearly competitive with centralized federated learning.  They believe future work could be improved via applying even more sophisticated peer sampling methods that are optimized  based on other tradeoffs.
Their experimental scenarios include a real churn trace collected over phones, both continuous and bursty communication patterns, different network sizes and different distributions of the training data over the devices. They also evaluate a number of additional techniques including a compression technique based on sampling, and token account based flow control for gossip learning. After evaluating the average cost of both approaches, they found that the best gossip variants perform comparably to the best federated learning variants overall. %[]
\cite{hu2019decentralized} initiated their study with a logical hypothesis is that gossip learning without an aggregation server or a central component will be strictly less efficient than federated learning due its reliance on a more basic infrastructure, just message passing and no cloud resources.  They state that, “One of the most challenging problem of federated learning is the poor network connection as the workers are geodistributed and connected with slow WAN.”  They asked the question if workers can even send full model updates (Size of BERT Large can be up to 1360MB).  They ask if “is it possible for workers to synchronize the model partially, from/to only a part of the workers, and still achieve good training results?” They explore this area with a decentralized FL solution, called Combo. Knowing that peer-to-peer bandwidth is much smaller than the worker’s maximum network capacity, their program could fully utilize the bandwidth by saturating the network with segmented gossip aggregation. Their experiments end up showing that they can reduce the training time significantly with great convergence performance. %[]
\cite{jameel2019ring} focus on the the design choices for a sparse model averaging strategy in a decentralized parallel SGD. Their attempt to design an optimal communication topology that is both quick and efficient.  They propose a superpeer topology where they form a ring and have some number of regular peers connected to them. The hierarchical two-layer sparse communication topology allows a principled trade-off between convergence speed and communication overhead and is well suited to loosely coupled distributed systems. We demonstrate this using an image classification task and a batch stochastic gradient descent learning (SGD) algorithm that their proposed method shows similar convergence behavior as Allreduce while having lower communication costs.  %[We plan on testing a super peer topology similar to this, we are also likely to test on an image classification CIFAR10]
Giaretta and Girdzijauskas \cite{giaretta2019gossip}  examine the conditions in which gossip learning can and cannot be applied.  Our team takes a hard look at the extensions that their research has mentioned to mitigate some of the limitations in FL.  They present a thorough analysis of the applicability of gossip learning, Their work includes scenarios that range from the effect of certain topologies, and the correlation of communication speed and data distribution. %[]
Although tested on Industrial IoT (IioT) setting, Savazzi et al. \cite{savazzi2020federated} study a handful of gossip based decentralized machine learning methods in the context of (IioT) apps. Their focus is on when the data distribution is not identical over the nodes (similar to privacy data on cell phones. They do not consider compression techniques or other algorithmic enhancements such as token-based flow control.
Their paper proposes a serverless learning approach, where the proposed FL algorithms use device to device cooperation to perform data operations on the  network by iterating local computations using consensus-based methods. %[We plan on evaluating their approach which lays the groundwork for integration of FL within 5G and beyond networks characterized by decentralized connectivity and computing.]
%%%%%%%%%%%%%%%%%%%%%%%%%%%

Most of the current FL systems are implemented using synchronous update, which is susceptible to the straggler effect. To address this problem, \cite{xie2019asynchronous} proposed an asynchronous algorithm for federated optimization, \textit{FedAsync}, which solves regularized local problems and then uses a weighted average with respect to the staleness to update the global model. \cite{chen2019asynchronous} presented \textit{ASO-Fed} for asynchronous online FL, which uses the same surrogate objective for local updates, while the local learning rate is adaptive to the average time cost of past iterations. 

\section{Approach}

%The concept of gossip learning and clustering Federated Learning could be combined to explore more improvement for FL.  FL could still be greatly benefited if peer-to-peer communication could be utilized in centralized federated learning.  Devices in the same cluster could do an ‘internal’ aggregation, then push the parameter to the central server for global aggregation. In this way, the communication between devices and the server could be reduced by utilizing more peer to peer communications between devices; also model accuracy is potentially increased by engaging more parameters participating the training in one epoch. Multiple approaches for device communications inside a cluster could be explored, like gossip, ring, and centralized leader. The scale of the cluster and the period to update cluster membership is also a trade off that can be explored. 


\subsection{Problem Formulation}
We consider the supervised federated learning problem which involves learning a single global statistical model owned by the central server, while each of $N$ devices owns a private dataset and works on training a local model. Let $\mathbf{w}$ parameterize the model, and $\mathcal{D}^i = \{\mathbf{x}_j, y_j\}$ denote the training dataset owned by $i$-th device, where $i\in\{1, \cdots, N\}$, $\mathbf{x}_j$ is the $j$-th input sample from $\mathcal{D}^i$, while $y_j$ is the corresponding label. Denote $\ell (\mathbf{x}_j, y_j | \mathbf{w})$ as the loss function presents the prediction error, our overall goal is to minimize the empirical loss $\mathcal{L}(\mathbf{w})$ over all distributed training data $\mathcal{D} = \bigcup_{i=1}^n \mathcal{D}^i$, i.e., we aim at solving the following optimization problem:
\[
\min\limits_\mathbf{w}  \mathcal{L}(\mathbf{w}) = \frac{ \sum_{i=1}^n \sum_{j\in \mathcal{D}^i} \ell (\mathbf{x}_j, y_j | \mathbf{w}) }{|\mathcal{D}|}.
\]
The problem is often solved by mini-batch stochastic gradient descent (SGD), in which during each step, the model is updated as $$\mathbf{w} \leftarrow \mathbf{w} - \alpha \dfrac{\partial \mathcal{L}}{\partial \mathbf{w}} ,$$ where $\alpha$ denotes the learning rate, and the average gradient $$\dfrac{\partial \mathcal{L}}{\partial \mathbf{w}}= \dfrac{1}{m} \sum_{j \in B} \dfrac{\partial \ell_j }{\partial \mathbf{w}} $$ is derived through back-propagation from the mini-batch $B$ of $m$ input samples. In the typical FL setting, each device $i$ performs SGD with data sampled from its own private training dataset $\mathcal{D}^i$ and train a local model $$\mathbf{w}_i = \text{arg}\min\limits_{\mathbf{w}_i} \mathcal{L}_i(\mathbf{w}_i) = \frac{ \sum_{j\in \mathcal{D}^i} \ell (\mathbf{x}_j, y_j | \mathbf{w}_i) }{|\mathcal{D}^i|} ,$$ 
the server aggregates all local models collected from the workers and updates the global model which is then sent back to the workers for the next iteration. 

\subsection{Proposed Method} \label{sec:our_method}

% \subsubsection{Learning on Central Server}

\subsubsection{Initialization at Central Server}% and Downlink Transmission}
We consider a hierarchcal FL system which has one central server on the cloud. The central server owns the global model $\mathbf{w}$ and denotes the timestamp $t$ for the model parameters. 
As the learning begins, the central server initializes the global model parameters $\mathbf{w}$, its timestamp $t=0$, as well as several hyperparameters that are required by learning. 
In addition, given the network information of all client devices, we allow the central server to be solely responsible for the knowledge of the hierarchical communication topology. 
In the case of mobile edge computing, the partition and hierarchy can be naturally formed by the communication edges, as the links between the central server with the edge servers or base stations form a star topology, and so do the links between each edge server with the devices. We extend this architecture to a more general case by allowing the central server to run clustering algorithm to assign which cluster each device belongs to, as well as a special device in each cluster, which we denote as the ``\textit{aggregator}'', that plays the role of an edge server, that is, it provides inter-hierarchy communication including downlink transmission from the central server and client devices and uplink transmission from the clients to the central server, while it also aggregates information to reduce the necessary communication. In FL, this aggregation work is specific to aggregating of the clients' updated weights/gradients, and sending them to the central server. The downlink transmission is straightforward: the server periodically sends the time stamped global model as well as the hyperparameters for the learning task to the aggregators, and the aggregator serves as the parent node of the client devices in each cluster, which forwards the information it receives from the central server to its children nodes.

\subsubsection{Learning on Local Clients}
Upon receiving the global model parameters $\mathbf{w}$ with its timestamp (according to the central server clock) from the central server, the worker client performs local update.
In order to mitigate the deviations of the local models on an arbitrary device $j$ from that of the central server, following \textit{FedAsync} \cite{xie2019asynchronous}, instead of minimization of the original local loss function $\ell_j$, client $j$ locally solves a regularized optimization problem, i.e., performs SGD update for one or multiple iterations on the following surrogate objective:
\[ \min\limits_{\mathbf{w}_j} g_j(\mathbf{w}_j) =  \mathbb{E}_{(\mathbf{x}, y) \sim \mathcal{D}^j} \Big[\ell_j(\mathbf{w}_j) + \frac{\lambda}{2} || \mathbf{w}_j - \mathbf{w} ||^2 \Big], \]
in which the regularization term $\frac{\lambda}{2} || \mathbf{w}_j - \mathbf{w} ||^2$ controls the deviation of the local models. After local learning, the client sends its updated parameters as well as the original model timestamp to its parent node, the corresponding aggregator. The next local learning iteration will based on the newest received global model and the corresponding timestamp.

\subsubsection{Learning on Cluster Aggregators}
On an aggregator, the rate of receiving updates from the clients varies because of heterogeneity of the computation power among devices, network delay, etc. We propose to perform asynchronous federated learning, that is, the aggregator immediately aggregates the update from the clients and reports to the central server. In real implementation, we can use a thread-safe FIFO queue to store the updates from the clients inside each aggregator, and periodically aggregate the results in the queue without waiting for potential stragglers. This is different from the synchronous FL paradigm, and the uplink communication is then non-blocking. Again, following \textit{FedAsync} \cite{xie2019asynchronous}, we use a function of staleness to mitigate the error caused by obsolete models. Intuitively, more staleness results in larger errors. On an aggregator device, assume the latest global model it received was with timestamp $t'$ (according to central server clock) at the moment it is about to aggregate the updates,
and the local model from client was with timestamp $t$, then it must be true that $t' \ge t$. We modify the learning rate to be weighted by the staleness:
\[ \alpha_{t'} = \alpha \times \sigma(t'-t), \]
in which $\sigma(z)$ is the staleness function. Different forms of $\sigma(z)$ were defined in \cite{xie2019asynchronous}, such as:
\begin{itemize}
    \item the polynomial form: \[ \sigma (t'-t) = (t'-t+1)^{-\beta} , \]
    \item the hinge form: \[ \sigma (t'-t) = \begin{cases}
    1      & \quad \text{if } t'-t \le b \\
    \dfrac{1}{a(t'-t-b)+1}  & \quad \text{otherwise}
  \end{cases} \]
\end{itemize}
Note that $\sigma(t'-t)=1$ if $t'=t$, and monotonically decreases as $t'$ and $t$ deviates more, so that the obsolete update would affect the model less as it shrinks the learning rate. 
Therefore, upon receiving local update from client device $j$, the model can then be updated on the cluster aggregator $k$ as:
\[ \mathbf{w}_k^{(t')} \leftarrow (1-\alpha_{t'}) \mathbf{w}^{(t')} + \alpha_{t'} \mathbf{w}^{(t)}_{j}. \]

Or equivalently, if we aggregate the gradients collected from the clients, we have:
\[ \mathbf{dw}_k^{(t')} = \sum \alpha \sigma(t'-t) \mathbf{dw}_j^{(t)}, \]
where $\mathbf{dw}_j^{(t)} $ is the gradient collected by device $j$ in $k$-th cluster. 

\subsubsection{Learning on Central Server}
The central server aggregates the results from the cluster aggregators to update the global model. Similar to the learning procedure on the cluster aggregators, in the central server, we can use a queue to store the updates from the aggregators. As asynchronous learning, the numbers of updates gathered from each of the aggregators can be imbalanced. As such, we let the aggregator $k$ send the number of updates $n_k$ along with the aggregated results (i.e., $\mathbf{w}_k^{(t')}$ and timestamp $t'$) to the central server. We assume the newest global model was updated at timestamp $t''$ according to the central server clock, and that it must hold $t'' > t'$. Combine the update counts information and the staleness schema, by collecting the update from aggregator $k$, the learning rate is weighted and modified as $$\alpha_{t''} = \dfrac{n_k}{N} \sigma(t''-t') \alpha, $$
where $N$ is the total number of devices in the FL system which is known to the central server. 
Note that this weighting mechanism makes sense if the data are i.i.d. over the clients. However, the bias could be severe if non-i.i.d. data are involved, as the mechanism favors learning for faster computed and communicated devices, in which case we need to carefully tune and design a more complicated weighting mechanism. The central server updates the global model as follows:
\[
    \mathbf{w}^{(t''+1)}  \leftarrow (1-\alpha_{t''}) \mathbf{w}^{(t'')} + \alpha_{t''} \mathbf{w}^{(t')}_{k},
\]

The detailed algorithm is illustrated in Algorithm \ref{alg:fedah}.

\begin{algorithm}[htb!]
\caption{Asynchronous Hierarchical Federated Learning (FedAH)}
\label{alg:fedah}
\begin{algorithmic}[1]
\footnotesize
\STATE \underline{\textbf{Central Server:}}
% \bindent
    \STATE Assign clusters and aggregators according to network topology or by running clustering algorithm.
    \STATE Initialize global model $\mathbf{w}$ and time clock $t$.
% \REPEAT[for each episode]
\FOR{$t=0, \cdots, T-1$ \textbf{until} end of learning}
    \STATE Broadcast $(\mathbf{w}, t)$ to its direct children aggregators. 
    \STATE Receive triples $\big(\mathbf{dw}_k^{(t')}, t', n_k\big)$ from any direct child aggregator $k$.
    \STATE Update global model %$\mathbf{w}^{(t+1)}  \leftarrow (1-\alpha_{t}) \mathbf{w}^{(t)} + \alpha_{t} \mathbf{w}^{(t')}_{k}$ 
    $\mathbf{w}^{(t+1)}  \leftarrow \mathbf{w}^{(t)} - \alpha_{t} \mathbf{dw}^{(t')}_{k}$  where $\alpha_{t} = \alpha \sigma(t-t') n_k / N$.
\ENDFOR
% \eindent
\STATE \underline{\textbf{Middle Layer Aggregator:}}
    \STATE Receive $\big(\mathbf{w}, t''\big)$ from its parent, broadcast to its direct children.
    \STATE Receive $\big(\mathbf{w}_j^{(t')}, t'\big)$ from any of its direct child $j$.
    \STATE Aggregate the collected gradients: $ \mathbf{dw}_k^{(t')} = \sum \alpha \sigma(t''-t') \mathbf{dw}_j^{(t)}$.
    \STATE Send triples $\big(\mathbf{dw}_k^{(t')}, t', n_k\big)$ to its parent.
    
\STATE \underline{\textbf{Bottom Layer Client Device:}}
    \STATE Receive $\big(\mathbf{w}, t''\big)$ from its parent.
    \STATE Define $g_j(\mathbf{w}_j) = \ell_j(\mathbf{w}_j) + \frac{\lambda}{2} || \mathbf{w}_j - \mathbf{w} ||^2 $
    \FOR{local iteration}
        \STATE Randomly sample $(\mathbf{x}, y) \sim \mathcal{D}^i$
        \STATE Local update $\mathbf{w}_j \leftarrow \mathbf{w}_j - \alpha \nabla g_j$
    \ENDFOR
    \STATE Send updated model $\big(\mathbf{w}_j, t''\big)$ to its parent.


\end{algorithmic}
\end{algorithm}

\subsection{Convergence Analysis}

\begin{definition}[Lipschitz Smoothness]
A function $f$ is Lipschitz smooth with constant $L>0$ if its derivatives are Lipschitz continuous with $L$, i.e., $\forall x, y$, 
\[ || \nabla f(y) - \nabla f(x) || \le L || y - x ||, \]
in other words, we have
\[ f(y) - f(x) \le \langle \nabla f(x), y-x \rangle + \frac{L}{2} ||y-x||^2. \]
\end{definition}

\begin{definition}[Strong Convexity]
A differentiable function $f$ is $\mu$-strongly convex with constant $\mu > 0$ if $\forall x, y$, 
\[ f(y) - f(x) \ge \langle \nabla f(x), y-x \rangle + \frac{\mu}{2} ||y-x||^2, \]
which is equivalent to the function $g(x) = f(x)-\frac{\mu}{2}||x||^2$ is convex for all $x$, while the latter is further equivalent to $\nabla ^2 g \succeq 0$, which is $\nabla ^2 f \succeq \mu I$.
\end{definition}

\begin{assumption}
The global loss function $\mathcal{L}$ is L-smooth, $\mu$-strongly convex, and bounded from below.  
\end{assumption}
\begin{assumption}
We assume bounded delay of communication and bounded processing time in the system.
\end{assumption}

\begin{lemma}[Asymptotic optimality]
Suppose Assumption 1 and 2 hold, after $T$ global updates on the server, Algorithm \ref{alg:fedah} converges to a critical point. 
\end{lemma}

\begin{lemma}[Convergence rate]
\[ \frac{ \mathcal{L}(\mathbf{w}^{(t+1)}) - \mathcal{L}(\mathbf{w}^\ast)  }{\mathcal{L}(\mathbf{w}^{(t)}) - \mathcal{L}(\mathbf{w}^\ast) } \le (1-\delta)^c \]
\end{lemma}

We will derive the values and proofs later.

\section{(Initial) Experiments and Results}

As the initial experiment, we consider an asynchronous hierarchical FL system with 20 client devices, 2 cluster aggregators, and a central server. The 22 non-server machines are grouped into two clusters using hierarchical agglomerate cluster based on their IP addresses while in reality, the computational power and network conditions can be integrated as the clustering features as well. For simplicity, the aggregators in each cluster are randomly selected. We conduct our initial experiments on a standard image classification task, the famous CIFAR-10 dataset was used. We set up the model as a convolutional neural network (CNN) with 3 convolutional blocks, which has 5852170 parameters and achieves 90\% test accuracy in centralized training. For our FL system, we randomly partition the CIFAR-10 dataset among the 20 local learning devices, so that each of the 10 class labels are kinds of balanced distributed over all clients. For local training, SGD optimizer are employed with a batch size of 128 and an initial learning rate of 0.001. We implement our model and learning procedure using PyTorch, while the communication between different devices are through sockets using TCP protocol. Specifically, in each process on a device, we established multiple threads to conduct different tasks: 
\begin{itemize}
    \item A \texttt{receiving} thread that listens to the socket to receive uplink/downlink messages from its parent or direct children, and decodes the information. In middle layer aggregators, two separate \texttt{receiving} threads are handling the messages from the central server and the clients, respectively.
    \item A \texttt{sending} thread that encodes the model parameters as well as other necessary information and sending to its parent or direct children. Again, in middle layer aggregators, two separate \texttt{sending} threads are used.
    \item A \texttt{learning} thread that performs either local update or aggregation tasks to update the model. 
    \item And an optional \texttt{testing} thread that evaluates the performance of the newest model in the device based on the local validation dataset. 
\end{itemize}

A separate thread-safe FIFO queue is used by each of the \texttt{receiving} threads to hold the data it receives. Read/Write locks are used when the \texttt{receiving} thread writes to the queue and the \texttt{learning} thread reads from the queue. This potentially prevents us from conducting experiments for more clients, say, 50. More sophisticated concurrency control mechanisms are needed, which we have planned as the work for the next step. 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.375\textwidth]{figs/midterm.png}
    \caption{Initial Experimental Results}
    \label{fig:midterm}
\end{figure}


% \textit{FedAvg}
% \textit{FedAsync}
% \textit{HierFedAvg}
% \textit{HierFedAsync}

As for the initial experiments, we compare the performance of several different algorithms with our proposed approach. %The non-hierarchical synchronous FL uses the standard \textit{FedAvg} setup, while the non-hierarchical asynchronous FL allows 
Figure \ref{fig:midterm} shows the comparison of the global models' accuracy evaluated on central servers' validation dataset verses the training time. In \textit{FedAvg} and \textit{FedAsync}, 20 worker devices directly communicate to the central server without hierarchical structure, while \textit{FedAsync} allows the server and workers to update the models at any time without synchronization. Our hierarchical learning involves the Client-Aggregator-Server 3-layer hierarchical structure, where \textit{HierFedAvg} perform synchronous learning, while \textit{HierFedAsync} is closer to the learning schema we described in Section \ref{sec:our_method}. 
In each setting, we let the learning last for 3000 seconds to collect the initial experiment results. 
Note that currently we have not integrated the proposed modification of the learning rate into our implementation. Specifically, after midterm review, we will allow the aggregation to be proportional to the numbers of updates in hierarchical learning, as well as adjust with the staleness function in asynchronous learning. We will also add the regularization term in loss function for local update in asynchronous settings. Yet we can already have some valuable observation and gain some intuition from our current results. Without emphasis on simulation of the stragglers, the asynchronous learning converges much faster than the synchronous paradigm, as they allow more computation occur on local workers. However, the learning curves of synchronous learning are much smoother, indicates that the asynchronous learning is not stable. In fact, this is exactly what our work aims at. We expect with the modification we mentioned in Section \ref{sec:our_method}, the learning curves of asynchronous learning would have less oscillation.
We also notice that by adding hierarchical structure, synchronous learning performance has a non-trivial decrease, unlike its asynchronous counterpart. We also expect the unstableness of hierarchical learning could be alleviated by deploying the modification in Section \ref{sec:our_method}. In fact, even if the accuracy performance of hierarchical learning does not get improved over the star-topology, although not shown here, the communication burden of the central server would be greatly alleviated in a hierarchical topology, not to mention the potential benefits of additional local computation and faster overall convergence. In a large system of network topology, this could also leads to less packet loss, more effective communication and computation. These are all the metrics we could benchmark on in the next step. We also note the modifications described in Section \ref{sec:our_method} involve careful derivation and tuning of several hyperparameters, comparative analysis would be conducted on different values of the hyperparameters as well, to show the effect of each modification we proposed.


%-------------------------------------------------------------------------
% \section{Conclusions and Future Work}
\section{Discussions}

In this paper, we proposed asynchronous hierarchical federated learning. 
As federated networks are composed of a massive number of devices, communication is a critical challenge in FL. We tackle this problem by exploring different architectural patterns for the design of FL systems. The tradeoff of central and fully decentralized learning on the complexity of the system as well as the learning effectiveness, computational and communication cost is obvious. We deploy a FL system with a central server, but with hierarchical topology. In this paper, we combine asynchronous FL and hierarchical FL into our \textit{FedAH} algorithm. In addition, we blur the concept of network topological edges to form clusters as well as the hierarchical structures. We aimed to reduce the communication load between devices and the server in FL system, and improve flexibility and scalability. Our initial experiments demonstrated that combining asynchronous FL and hierarchical FL not only leads to faster convergence, tolerates heterogeneity of the system such as the straggler effect, also significantly alleviates the communication burden on the central server. However, the asynchronous and hierarchical nature greatly increases the complexity of the system, especially on the communication topology, which could lead to unstable learning. 
We explored the literature and recent research advances, combined them into our proposed method, and are hopeful that it could mitigate the instability. The fully implemented system will be evaluated after the midterm.

%Clustering Federated Learning could be potentially used to form an optimal system pattern with peer-to-peer communication between devices utilized to reduce the communication load between devices and the server.

%The central server uses either the network topology or some clustering algorithm to assign clusters for workers (i.e., client devices). In each cluster, a special aggregator device is selected to enable hierarchical learning, leads to efficient communication between server and workers, so that the burden of the server can be significantly reduced. In addition, asynchronous federated learning schema is used to tolerate heterogeneity of the system and achieve fast convergence, i.e., the server aggregates the gradients from the workers weighted by a staleness parameter to update the global model, and regularized stochastic gradient descent is performed in workers. 

%------------------------------------------------------------------------

%-------------------------------------------------------------------------


% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

{\small
\bibliographystyle{abbrvnat}
\bibliography{FlRef}
}

\end{document}
